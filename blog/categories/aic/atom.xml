<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Read more on: aic | asb: head /dev/brain > /dev/www]]></title>
  <link href="http://akhilsbehl.github.io/blog/categories/aic/atom.xml" rel="self"/>
  <link href="http://akhilsbehl.github.io/"/>
  <updated>2016-05-30T17:09:52+05:30</updated>
  <id>http://akhilsbehl.github.io/</id>
  <author>
    <name><![CDATA[Akhil S. Behl]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The corrected AIC]]></title>
    <link href="http://akhilsbehl.github.io/blog/2014/03/22/the-corrected-aic/"/>
    <updated>2014-03-22T23:35:53+05:30</updated>
    <id>http://akhilsbehl.github.io/blog/2014/03/22/the-corrected-aic</id>
    <content type="html"><![CDATA[<p>Only today I discovered that the Akaike Information Criterion is valid only
<em>asymptotically</em> and that there exists a correction (in fact, a strongly
recommended correction) for finite samples. Here is a quick copy-paste from
<a href="http://en.wikipedia.org/wiki/Akaike_information_criterion">Wikipedia</a>.</p>

<!--more-->

<p><blockquote><p>AICc is AIC with a correction for finite sample sizes:</p></p><p><script type="math/tex; mode=display">AICc = AIC + \frac{2k(k + 1)}{n - k - 1}</script></p><p><p>where n denotes the sample size. Thus, AICc is AIC with a greater penalty for<br/>extra parameters.</p></p><p><p>Burnham &amp; Anderson (2002) strongly recommend using AICc, rather than AIC, if n<br/>is small or k is large. Since AICc converges to AIC as n gets large, AICc<br/>generally should be employed regardless. Using AIC, instead of AICc, when n is<br/>not many times larger than k2, increases the probability of selecting models<br/>that have too many parameters, i.e. of overfitting. The probability of AIC<br/>overfitting can be substantial, in some cases.</p></p><p><p>Brockwell &amp; Davis (1991, p. 273) advise using AICc as the primary criterion in<br/>selecting the orders of an ARMA model for time series. McQuarrie &amp; Tsai (1998)<br/>ground their high opinion of AICc on extensive simulation work with regression<br/>and time series.</p></p><p><p>AICc was first proposed by Hurvich &amp; Tsai (1989). Different derivations of it<br/>are given by Brockwell &amp; Davis (1991), Burnham &amp; Anderson, and Cavanaugh<br/>(1997). All the derivations assume a univariate linear model with normally<br/>distributed errors (conditional upon regressors); if that assumption does not<br/>hold, then the formula for AICc will usually change. Further discussion of<br/>this, with examples of other assumptions, is given by Burnham &amp; Anderson (2002,<br/>ch. 7). In particular, bootstrap estimation is usually feasible.</p></p><p><p>Note that when all the models in the candidate set have the same k, then AICc<br/>and AIC will give identical (relative) valuations. In that situation, then, AIC<br/>can always be used.</p></blockquote></p>

<p>It should be trivial to write this correction oneself in any language. <a href="http://www.inside-r.org/packages/cran/sme/docs/AICc">Here</a> and
<a href="http://www.awblocker.com/R/AICc.R">here</a> are a couple of such examples in R.</p>

<p>PS: I also learnt how to use LaTeX in octopress from <a href="http://www.idryman.org/blog/2012/03/10/writing-math-equations-on-octopress/">here</a>. Thanks
to the author of this post.</p>

<!--links-->
]]></content>
  </entry>
  
</feed>
