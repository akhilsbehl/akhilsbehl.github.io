<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Read more on: aic | asb: head /dev/brain > /dev/www]]></title>
  <link href="http://akhilsbehl.github.io/blog/categories/aic/atom.xml" rel="self"/>
  <link href="http://akhilsbehl.github.io/"/>
  <updated>2016-10-17T11:26:38+05:30</updated>
  <id>http://akhilsbehl.github.io/</id>
  <author>
    <name><![CDATA[Akhil S. Behl]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The corrected AIC]]></title>
    <link href="http://akhilsbehl.github.io/blog/2014/03/22/the-corrected-aic/"/>
    <updated>2014-03-22T23:35:53+05:30</updated>
    <id>http://akhilsbehl.github.io/blog/2014/03/22/the-corrected-aic</id>
    <content type="html"><![CDATA[<p>Only today I discovered that the Akaike Information Criterion is valid only
<em>asymptotically</em> and that there exists a correction (in fact, a strongly
recommended correction) for finite samples. Here is a quick copy-paste from
<a href="http://en.wikipedia.org/wiki/Akaike_information_criterion">Wikipedia</a>.</p>

<!--more-->

<p>{% blockquote %}
AICc is AIC with a correction for finite sample sizes:</p>

<script type="math/tex; mode=display">AICc = AIC + \frac{2k(k + 1)}{n - k - 1}</script>

<p>where n denotes the sample size. Thus, AICc is AIC with a greater penalty for
extra parameters.</p>

<p>Burnham &amp; Anderson (2002) strongly recommend using AICc, rather than AIC, if n
is small or k is large. Since AICc converges to AIC as n gets large, AICc
generally should be employed regardless. Using AIC, instead of AICc, when n is
not many times larger than k2, increases the probability of selecting models
that have too many parameters, i.e. of overfitting. The probability of AIC
overfitting can be substantial, in some cases.</p>

<p>Brockwell &amp; Davis (1991, p. 273) advise using AICc as the primary criterion in
selecting the orders of an ARMA model for time series. McQuarrie &amp; Tsai (1998)
ground their high opinion of AICc on extensive simulation work with regression
and time series.</p>

<p>AICc was first proposed by Hurvich &amp; Tsai (1989). Different derivations of it
are given by Brockwell &amp; Davis (1991), Burnham &amp; Anderson, and Cavanaugh
(1997). All the derivations assume a univariate linear model with normally
distributed errors (conditional upon regressors); if that assumption does not
hold, then the formula for AICc will usually change. Further discussion of
this, with examples of other assumptions, is given by Burnham &amp; Anderson (2002,
ch. 7). In particular, bootstrap estimation is usually feasible.</p>

<p>Note that when all the models in the candidate set have the same k, then AICc
and AIC will give identical (relative) valuations. In that situation, then, AIC
can always be used.
{% endblockquote %}</p>

<p>It should be trivial to write this correction oneself in any language. <a href="http://www.inside-r.org/packages/cran/sme/docs/AICc">Here</a> and
<a href="http://www.awblocker.com/R/AICc.R">here</a> are a couple of such examples in R.</p>

<p>PS: I also learnt how to use LaTeX in octopress from <a href="http://www.idryman.org/blog/2012/03/10/writing-math-equations-on-octopress/">here</a>. Thanks
to the author of this post.</p>

<!--links-->
]]></content>
  </entry>
  
</feed>
