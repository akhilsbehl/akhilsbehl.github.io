<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Read more on: data.table | asb: head /dev/brain > /dev/www]]></title>
  <link href="http://akhilsbehl.github.io/blog/categories/data-dot-table/atom.xml" rel="self"/>
  <link href="http://akhilsbehl.github.io/"/>
  <updated>2016-09-07T19:36:24+05:30</updated>
  <id>http://akhilsbehl.github.io/</id>
  <author>
    <name><![CDATA[Akhil S. Behl]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[R: Converting a data.table to a multi way array (cube)]]></title>
    <link href="http://akhilsbehl.github.io/blog/2014/08/20/r-converting-a-data-dot-table-to-a-multi-way-array-cube/"/>
    <updated>2014-08-20T15:42:00+05:30</updated>
    <id>http://akhilsbehl.github.io/blog/2014/08/20/r-converting-a-data-dot-table-to-a-multi-way-array-cube</id>
    <content type="html"><![CDATA[<p>This post discusses the problem of converting a <code>data.table</code> to the <code>array</code> data structure in R. The idea is analogous to converting a denormalized dataset that presents both dimensions and facts in a table as columns of the table to a completely normalized fact cube along the dimensions of the dataset.</p>

<p>The problem can be solved in multiple ways in R with attending constraints of these approaches – e.g. <code>plyr::daply</code>, <code>xtabs</code>, <code>by</code> or a manual home-brewn set of split and lapply routine. Without discussing the constraints I observed with the existing techniques, I am presenting an alternative approach here that depends on unrolling the rectangular data structure into a linear structure and then reshaping it by manually counting the facts and dimension sizes (think strides). The choice of using a <code>data.table</code> was purely for efficiency reasons but the same idea can be implemented with a <code>data.frame</code> with little changes to the code.</p>

<!--more-->

<p>Dislcaimer: A constraint (I see it as essentially the functionality being implemented here) that all rows should be unique along the chosen dimensions. Since a cube must have all dimensions perfectly normalized.</p>

<p>Following is the implemenation with an example:</p>

<p>```r</p>

<p>dt2array = function (x, facts, dims) {
  stopifnot(is.data.table(x))
  setkeyv(x, rev(dims))
  stopifnot(!any(duplicated(x)))
  dimensions = lapply(x[ , rev(dims), with=FALSE],
                      function (x) sort(unique(x)))
  xFull = data.table(expand.grid(dimensions, stringsAsFactors=FALSE))
  setkeyv(xFull, rev(dims))
  x = data.table:::merge.data.table(xFull, x, by=dims, all=TRUE)
  factsVec = unlist(x[ , facts, with=FALSE], recursive=FALSE, use.names=FALSE)
  nFacts = length(facts)
  nDims = length(dims)
  if (nFacts &gt; 1) {
    dim(factsVec) = c(sapply(dimensions, length), nFacts)
    dimnames(factsVec) = c(dimensions, “facts”=list(facts))
    return(aperm(factsVec, perm=c(nDims:1, nDims + 1)))
  } else {
    dim(factsVec) = sapply(dimensions, length)
    dimnames(factsVec) = dimensions
    return(aperm(factsVec))
  }
}</p>

<p>dat = data.table(f1=runif(10),
                 f2=runif(10),
                 f3=runif(10),
                 d1=letters[1:5],
                 d2=rep(letters[3:4], each=5),
                 d3=LETTERS[1:2])
dt2array(dat, “f1”, c(“d1”, “d2”, “d3”))
dt2array(dat, c(“f1”, “f2”), c(“d1”, “d2”, “d3”))
dt2array(dat, c(“f1”, “f2”, “f3”), c(“d1”, “d2”, “d3”))</p>

<p>```</p>

<p>The implementation is very fast, if I say so myself, since almost all manipulations being used here are fairly low-level in R implemented in C. Hopefully, this will be useful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R: dplyr vs. data.table]]></title>
    <link href="http://akhilsbehl.github.io/blog/2014/02/13/r-dplyr-vs-data-dot-table/"/>
    <updated>2014-02-13T15:10:51+05:30</updated>
    <id>http://akhilsbehl.github.io/blog/2014/02/13/r-dplyr-vs-data-dot-table</id>
    <content type="html"><![CDATA[<p><a href="http://had.co.nz">Hadley Wickham</a>, with <a href="http://romainfrancois.blog.free.fr/">Romain Francois</a> and <a href="http://dirk.eddelbuettel.com/">Dirk Eddelbuettel</a>, has released a new package called <a href="http://cran.r-project.org/web/packages/dplyr">dplyr</a>. Once again Wickham has redesigned an important component of what one does with R. The package is designed specifically for rectangular datasets which may live in memory or databases. With the new package, Hadley also convincingly closes the efficiency gap between <a href="http://cran.r-project.org/web/packages/plyr">plyr</a> and <a href="http://cran.r-project.org/web/packages/data.table">data.table</a>. This new efficiency comes largely from the various C++ injections into the performance critical branches of code in <code>dplyr</code> (thanks to Romain and Eddelbuettel).</p>

<p>On the other hand <code>data.table</code>, a mature and popular project, started by <a href="http://stackoverflow.com/users/403310/matt-dowle">Matthew Dowle</a> has been the go-to for all performance-centric R programmers for some time now. While <code>dplyr</code> raises a serious contention to <code>data.table</code>’s claim to fame, both <code>data.table</code> and Dowle are old hands at such competition. There has long been (very healthy) competition between Hadley’s <code>plyr</code>, Dowle’s <code>data.table</code>, and <a href="http://blog.wesmckinney.com/">Wes McKinney</a>’s <code>pandas</code> (a data munging library for Python).</p>

<p>In this post I add another data point to the set of benchmarks of the two packages. For the official take, see <a href="http://cran.r-project.org/web/packages/dplyr/vignettes/benchmark-baseball.html">this</a> and <a href="http://arunsrinivasan.github.io/dplyr_benchmark/">this</a>.</p>

<!--more-->

<h2 id="the-set-up">The set up</h2>

<p>Because both <code>dplyr</code> and <code>data.table</code> have been written with efficiency for big datasets in mind, we must consider at least a moderately sized dataset for the benchmarks. Based on memory and patience I had available while writing the benchmarks, I created this synthetic dataset for the benchmarking.</p>

<p>```r Creating a synthetic sample</p>

<p>R&gt; set.seed(pi)
R&gt; sampSize = 1e7
R&gt; samp = data.frame(x=runif(sampSize, 2, 4),
+                    y=rnorm(sampSize, mean=runif(1), sd=runif(1, 1, 2)),
+                    z=letters[sample.int(26, sampSize, TRUE)],
+                    w=LETTERS[sample.int(26, sampSize, TRUE)],
+                    stringsAsFactors=FALSE)</p>

<p>```</p>

<p>The following is the number of steps we want to perform to transform this sample into a (not-so-much) useful summary.</p>

<ol>
  <li>Filter the samp to include only the first twenty of <code>letters</code> and the last twenty of <code>LETTERS</code>.</li>
  <li>Select only columns x, y, and z out of the data.frame.</li>
  <li>Create two new columns:
    <ol>
      <li>xProp = x / sum(x)</li>
      <li>yScale = (y - mean(y)) / sd(y)</li>
    </ol>
  </li>
  <li>Calculate mean(xProp) and mean(yScale) by z.</li>
  <li>Arrange / order output by z.</li>
</ol>

<h2 id="expression">Expression</h2>

<p>```r Base R</p>

<p>R&gt; baseR = samp[samp[[“z”]] %in% letters[1:20] &amp;
+               samp[[“w”]] %in% LETTERS[7:26],
+               c(“x”, “y”, “z”)]
R&gt; baseR[[“xProp”]] = with(baseR, x / sum(x))
R&gt; baseR[[“yScale”]] = with(baseR, (y - mean(y)) / sd(y))
R&gt; baseR = by(baseR, baseR[[“z”]],
+             function (x)
+               c(“meanXProp”=mean(x[[“xProp”]]),
+                 “meanYScale”=mean(x[[“yScale”]])))
R&gt; baseR = do.call(“rbind”, baseR)
R&gt; baseR
     meanXProp    meanYScale
a 1.688958e-07 -0.0031065551
b 1.690335e-07  0.0010690687
c 1.689844e-07 -0.0006084212
d 1.689843e-07 -0.0011474096
e 1.690749e-07 -0.0004805666
f 1.690463e-07  0.0028620687
g 1.690087e-07  0.0025418694
h 1.690328e-07 -0.0019483662
i 1.690802e-07 -0.0016024273
j 1.689372e-07  0.0031399568
k 1.689208e-07 -0.0026173210
l 1.690398e-07  0.0001232977
m 1.690471e-07  0.0011827109
n 1.688495e-07  0.0008292519
o 1.689488e-07  0.0019278568
p 1.690026e-07  0.0011351928
q 1.690664e-07 -0.0008938230
r 1.689304e-07 -0.0001913007
s 1.689704e-07 -0.0020868491
t 1.689081e-07 -0.0001515275</p>

<p>```</p>

<p>```r dplyr</p>

<p>R&gt; dply =
+    samp %.%
+    filter(z %in% letters[1:20], w %in% LETTERS[7:26]) %.%
+    select(x, y, z) %.%
+    mutate(xProp=x / sum(x),
+           yScale=(y - mean(y)) / sd(y)) %.%
+    group_by(z) %.%
+    summarise(meanXProp=mean(xProp), meanYScale=mean(yScale)) %.%
+    arrange(z)
R&gt; dply
   z    meanXProp    meanYScale
1  a 1.688958e-07 -0.0031065551
2  b 1.690335e-07  0.0010690687
3  c 1.689844e-07 -0.0006084212
4  d 1.689843e-07 -0.0011474096
5  e 1.690749e-07 -0.0004805666
6  f 1.690463e-07  0.0028620687
7  g 1.690087e-07  0.0025418694
8  h 1.690328e-07 -0.0019483662
9  i 1.690802e-07 -0.0016024273
10 j 1.689372e-07  0.0031399568
11 k 1.689208e-07 -0.0026173210
12 l 1.690398e-07  0.0001232977
13 m 1.690471e-07  0.0011827109
14 n 1.688495e-07  0.0008292519
15 o 1.689488e-07  0.0019278568
16 p 1.690026e-07  0.0011351928
17 q 1.690664e-07 -0.0008938230
18 r 1.689304e-07 -0.0001913007
19 s 1.689704e-07 -0.0020868491
20 t 1.689081e-07 -0.0001515275</p>

<p>```</p>

<p>```r data.table</p>

<p>R&gt; dtSamp = data.table(samp)
R&gt; dt = dtSamp[z %in% letters[1:20] &amp; w %in% LETTERS[7:26], list(x, y, z)]
R&gt; dt = dt[ , list(xProp=(x / sum(x)), yScale=(y - mean(y)) / sd(y), z)]
R&gt; dt = dt[ , list(meanXProp=mean(xProp), meanYScale=mean(yScale)), by=z]
R&gt; dt = dt[order(dt[[“z”]]), ]
R&gt; dt
    z    meanXProp    meanYScale
 1: a 1.688958e-07 -0.0031065551
 2: b 1.690335e-07  0.0010690687
 3: c 1.689844e-07 -0.0006084212
 4: d 1.689843e-07 -0.0011474096
 5: e 1.690749e-07 -0.0004805666
 6: f 1.690463e-07  0.0028620687
 7: g 1.690087e-07  0.0025418694
 8: h 1.690328e-07 -0.0019483662
 9: i 1.690802e-07 -0.0016024273
10: j 1.689372e-07  0.0031399568
11: k 1.689208e-07 -0.0026173210
12: l 1.690398e-07  0.0001232977
13: m 1.690471e-07  0.0011827109
14: n 1.688495e-07  0.0008292519
15: o 1.689488e-07  0.0019278568
16: p 1.690026e-07  0.0011351928
17: q 1.690664e-07 -0.0008938230
18: r 1.689304e-07 -0.0001913007
19: s 1.689704e-07 -0.0020868491
20: t 1.689081e-07 -0.0001515275</p>

<p>```</p>

<p>IMHO, the syntax for both <code>data.table</code> and <code>dplyr</code> is cleaner and more consistent for the kind of operations considered. But this is unsurprising because both are Domain Specific Languages (DSLs) built specificlly for this limited functionality. It is also annoying that both syntaxes confuse Vim and demand manual formatting.</p>

<p>Both <code>dplyr</code> and <code>data.table</code> also accept column names as free variables in expressions. Of the following three expressions,</p>

<p>```r</p>

<p>x[z %in% letters[1:20]]
x[x[[“z”]] %in% letters[1:20]]
with(x, x[z %in% letters[1:20]])</p>

<p>```</p>

<p>I find the second form more elegant than the first. Although, personally, I also find the third <em>most correct in intent</em>.</p>

<p>If I were to judge these three on syntax, I’d probably choose <code>dplyr</code> for it’s design of what is now being called a <a href="http://blog.revolutionanalytics.com/2014/01/fast-and-easy-data-munging-with-dplyr.html"><em>grammar of data manipulation</em></a>. <code>data.table</code> on the other hand is notorious for being so far removed from traditional R (and everything else) that even advanced R users may find it tiresome. Despite the tiring nature of the new syntax, it is quite dense in expression (perhaps a little too dense). I need more experience with <code>dplyr</code> to be able to find situations where this sub-language struggles in expression.</p>

<h2 id="efficiency-benchmarks">Efficiency benchmarks</h2>

<p>Here is the code used to benchmark the three ways of expressing the desired data manipulation:</p>

<p>```r</p>

<p>R&gt; set.seed(pi)
R&gt; samp = data.frame(x=runif(1e7, 2, 4),
+                      y=rnorm(1e7, mean=runif(1), sd=runif(1, 1, 2)),
+                      z=letters[sample.int(26, 1e7, TRUE)],
+                      w=LETTERS[sample.int(26, 1e7, TRUE)],
+                      stringsAsFactors=FALSE)</p>

<h1 id="because-i-think-this-should-be-out-of-the-benchmark">Because I think this should be out of the benchmark.</h1>
<p>R&gt; dtSamp = data.table(samp)</p>

<p>R&gt; mbc = microbenchmark({
+    baseR = samp[samp[[“z”]] %in% letters[1:20] &amp;
+                 samp[[“w”]] %in% LETTERS[7:26],
+                 c(“x”, “y”, “z”)]
+    baseR[[“xProp”]] = with(baseR, x / sum(x))
+    baseR[[“yScale”]] = with(baseR, (y - mean(y)) / sd(y))
+    baseR = by(baseR, baseR[[“z”]],
+               function (x)
+                 c(“meanXProp”=mean(x[[“xProp”]]),
+                   “meanYScale”=mean(x[[“yScale”]])))
+    baseR = do.call(“rbind”, baseR)
+  },
+  {
+    dply =
+    samp %.%
+    filter(z %in% letters[1:20], w %in% LETTERS[7:26]) %.%
+    select(x, y, z) %.%
+    mutate(xProp=x / sum(x),
+           yScale=(y - mean(y)) / sd(y)) %.%
+    group_by(z) %.%
+    summarise(meanXProp=mean(xProp), meanYScale=mean(yScale)) %.%
+    arrange(z)
+  },
+  {
+    dt = dtSamp[z %in% letters[1:20] &amp; w %in% LETTERS[7:26], list(x, y, z)]
+    dt = dt[ , list(xProp=(x / sum(x)), yScale=(y - mean(y)) / sd(y), z)]
+    dt = dt[ , list(meanXProp=mean(xProp), meanYScale=mean(yScale)), by=z]
+    dt = dt[order(dt[[“z”]]), ]
+  },
+  times=10L)</p>

<p>R&gt; print(mbc) # Ignoring the expressions:
     min       lq   median       uq      max neval
7.444853 7.688928 7.735172 7.801475 7.932713    10
2.217018 2.293009 2.330963 2.425163 2.506184    10
2.847840 2.926100 2.944250 3.025527 3.071644    10</p>

<p>```</p>

<p>On a moderately big dataset and a realistic computation, both <code>data.table</code> and <code>dplyr</code> are pretty efficient and pretty similar. However, base R is not orders of magnitude worse. Tight code in base R is still quite competitive; the rub is that it takes significant effort to write tight R code.</p>

<p>The gap between <code>data.table</code> and <code>dplyr</code> is too small at the moment to tip the scales in anyone’s favor. This essentially corroborates the official benchmarks from the <code>dplyr</code> and <code>data.table</code> communities. However, I am also looking forward to validating the claim that <code>data.table-1.8.11</code> <em>will push the bar higher</em>. Hadley has made similar comments about extending <code>dplyr</code> and making it more efficient. Another noteworthy remark is that while <code>data.table</code> is at stable version <code>1.8.0</code>, <code>dplyr</code> is at current version <code>0.1</code>. This definitely makes the latter the new cool kid on the block.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Given that software shows significant switching (learning) costs and network benefits, this contest becomes very interesting. There are comparably sized <code>plyr</code> and <code>data.table</code> communities already. Both packages are different enough (from each other and base R) that they are inherently incompatible standards (though dplyr is trying to subsume data.table). The individual popularity of the two lead developers, Hadley and Dowle, may play a role. I believe that this competition will not be over soon. I am also quite certain that this competition between the incumbent and the challenger will create abundunant surplus for the users of R. Amen!</p>

<p>Here is the code <a href="https://gist.github.com/akhilsbehl/8973994">gist</a>.</p>

<!--links-->
]]></content>
  </entry>
  
</feed>
