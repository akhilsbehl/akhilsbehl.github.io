
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Why you have, in fact, used a neural network! - asb: head /dev/brain > /dev/www</title>
  <meta name="author" content="Akhil S. Behl">

  
  <meta name="description" content="Have you ever used a logistic regression? Would you like to be able to say you have used a neural network – perhaps in your next interview? Here’s &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://akhilsbehl.github.io/blog/2016/09/06/why-you-have-in-fact-used-a-neural-network">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://feeds.feedburner.com/github/Xmaq" rel="alternate" title="asb: head /dev/brain > /dev/www" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-42442765-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">asb: head /dev/brain > /dev/www</a></h1>
  
    <h2>My home, musings, and wanderings on the world wide web.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss">
  <li><a href="http://feeds.feedburner.com/github/Xmaq" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
</ul>
  
  
  
  
  
<ul class="subscribe">
  <li><a href="https://github.com/akhilsbehl" rel="subscribe-github" title="@akhilsbehl on GitHub" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 50,0 C 22.385714,0 0,22.385714 0,50 0,77.614286 22.385714,100 50,100 77.614286,100 100,77.614286 100,50 100,22.385714 77.614286,0 50,0 z m 29.692858,79.692858 c -3.859184,3.859182 -8.351022,6.887754 -13.35,9.00306 -1.27041,0.536736 -2.560204,1.009184 -3.867348,1.415306 v -7.493878 c 0,-3.938774 -1.35102,-6.835714 -4.053062,-8.690816 1.692858,-0.163264 3.24694,-0.390816 4.663266,-0.683672 1.416326,-0.292858 2.913266,-0.716328 4.491838,-1.27041 1.57857,-0.55408 2.994896,-1.213264 4.247958,-1.97755 1.253062,-0.765306 2.458164,-1.758164 3.613266,-2.978572 1.155102,-1.220408 2.12449,-2.604082 2.905102,-4.15 0.780612,-1.545918 1.4,-3.40204 1.855102,-5.566326 0.455102,-2.164286 0.683674,-4.54898 0.683674,-7.153062 0,-5.045918 -1.643878,-9.341836 -4.931634,-12.890816 C 77.44796,33.35 77.285714,29.10204 75.463266,24.512244 l -1.22143,-0.145918 c -0.845918,-0.09796 -2.368366,0.260204 -4.565306,1.07449 -2.196938,0.814286 -4.663264,2.14796 -7.396938,4.004082 -3.87449,-1.07449 -7.893878,-1.611224 -12.061224,-1.611224 -4.19898,0 -8.203062,0.536734 -12.012246,1.611224 -1.72449,-1.17245 -3.361224,-2.139796 -4.907142,-2.905102 C 31.753062,25.77449 30.516326,25.254082 29.587756,24.97653 28.660204,24.7 27.79796,24.528572 27,24.463266 c -0.79796,-0.0653 -1.310204,-0.08062 -1.537756,-0.04898 -0.22755,0.03164 -0.390816,0.0653 -0.487754,0.09796 -1.82347,4.62245 -1.985714,8.87143 -0.487756,12.743878 -3.287754,3.54796 -4.931632,7.844898 -4.931632,12.890816 0,2.604082 0.227552,4.988776 0.683674,7.153062 0.456122,2.164286 1.07449,4.020408 1.855102,5.566326 0.780612,1.545918 1.75,2.929592 2.905102,4.15 1.155102,1.220408 2.360204,2.213266 3.613264,2.978572 1.253062,0.766326 2.669388,1.42449 4.24796,1.97755 1.578572,0.554082 3.07551,0.976532 4.491836,1.27041 1.416328,0.292856 2.970408,0.521428 4.663266,0.683672 -2.669388,1.82347 -4.004082,4.720408 -4.004082,8.690816 v 7.639796 C 36.536734,89.818368 35.083674,89.3 33.656122,88.695918 c -4.99898,-2.115306 -9.490816,-5.143878 -13.35,-9.00306 -3.859184,-3.859184 -6.887754,-8.351022 -9.00306,-13.35 C 9.1163263,61.171428 8.0071428,55.67347 8.0071428,50 c 0,-5.67347 1.1091835,-11.171428 3.2969392,-16.342858 2.115306,-4.998978 5.143878,-9.490816 9.00306,-13.35 3.859184,-3.859182 8.351022,-6.887754 13.35,-9.00306 C 38.828572,9.1163266 44.32653,8.0071428 50,8.0071428 c 5.67347,0 11.171428,1.1091838 16.342858,3.2969392 5,2.115306 9.490816,5.143878 13.35,9.00306 3.859182,3.859184 6.887754,8.351022 9.00306,13.35 2.186736,5.17245 3.295918,10.67041 3.295918,16.342858 0,5.672448 -1.109182,11.171428 -3.296938,16.342858 -2.115306,4.998978 -5.143878,9.490816 -9.00204,13.35 l 0,0 z"></path></svg></a></li>
</ul>
  
  
  
  
<ul class="subscribe">
  <li><a href="https://linkedin.com/in/akhilsbehl" rel="subscribe-linkedin" title="Akhil S. Behl on LinkedIn" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewBox="0 0 731 1000"><path class="social" d="M158.75 865l-156.25 0l0 -521.25l156.25 0l0 521.25zm393.75 -505q87.5 0 133.125 56.25t45.625 153.75l0 295l-156.25 0l0 -278.75q0 -108.75 -76.25 -108.75 -61.25 0 -80 61.25l0 326.25l-156.25 0q2.5 -468.75 0 -521.25l123.75 0l10 103.75l2.5 0q53.75 -87.5 153.75 -87.5zm-552.5 -146.25q0 -78.75 81.25 -78.75 80 0 80 78.75 0 77.5 -80 77.5 -81.25 0 -81.25 -77.5z"/></svg></a></li>
</ul>
  
  
<ul class="subscribe">
  <li><a href="http://stackoverflow.com/users/1533691" rel="subscribe-stackoverflow" title="Akhil S. Behl on StackOverflow" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewBox="0 0 64 64"><g
     id="layer1" class="social">
    <path
       d="m 9.3049611,36.847632 4.4013079,0.04316 -0.153442,19.598393 29.291259,0 0,-19.527506 4.637782,0 0,24.287331 -38.2006795,0 0.023777,-24.401371 z"
       id="path2830"
       style="fill-opacity:1;stroke:none"/>
    <rect
       width="22.944817"
       height="4.881876"
       x="16.481951"
       y="48.434078"
       id="rect3604"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="20.749949"
       y="37.830307"
       transform="matrix(0.9953749,0.09606666,-0.09606666,0.9953749,0,0)"
       id="rect3606"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="26.472515"
       y="23.401554"
       transform="matrix(0.96240291,0.27162592,-0.27162592,0.96240291,0,0)"
       id="rect3606-1"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="30.528769"
       y="3.1535864"
       transform="matrix(0.85597805,0.51701216,-0.51701216,0.85597805,0,0)"
       id="rect3606-1-3"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="27.191883"
       y="-24.475019"
       transform="matrix(0.58242689,0.81288309,-0.81288309,0.58242689,0,0)"
       id="rect3606-1-3-7"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="11.174569"
       y="-50.183693"
       transform="matrix(0.16480989,0.98632535,-0.98632535,0.16480989,0,0)"
       id="rect3606-1-3-7-6"
       style="fill-opacity:1;stroke:none" />
  </g></svg></a></li>
</ul>
  
  
  
  
<ul class="subscribe">
  <li><a href="https://plus.google.com/u/0/101609335408319028285" rel="publisher" title="Google+ Profile" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 23.03264,55.19021 C 32.01805,55.457578 38.046082,46.137447 36.495958,34.370183 34.943794,22.603939 26.398215,13.161349 17.411784,12.89296 8.4253533,12.625592 2.3973217,21.630392 3.9494863,33.400718 5.5006303,45.164921 14.043148,54.921821 23.03264,55.19021 z M 99.99898,24.999027 100,0 -0.00797206,0 0.0052943,16.202408 C 5.7047282,11.184661 13.611481,6.9914696 21.771315,6.9914696 c 8.721103,0 34.888495,0 34.888495,0 l -7.807765,6.6035874 -11.062106,0 c 7.33732,2.81349 11.246815,11.3407 11.246815,20.090377 0,7.348545 -4.082979,13.667416 -9.852826,18.161652 -5.629021,4.385043 -6.696453,6.220904 -6.696453,9.948752 0,3.180866 6.030073,8.594563 9.183386,10.81923 9.217061,6.498477 12.199952,12.530591 12.199952,22.603843 0,1.604209 -0.198996,3.206378 -0.591884,4.780993 L 100,100 c 0,0 0,-46.594917 0,-68.751495 l -18.750474,0 0,18.750475 -6.250499,0 0,-18.750475 -18.750474,0 0,-6.250498 18.750474,0 0,-18.7494538 6.250499,0 0,18.7494538 18.750474,0 z M 18.147557,74.798916 c 2.111393,0 4.04522,-0.05817 6.048441,-0.05817 -2.651232,-2.571634 -4.749358,-5.722905 -4.749358,-9.606889 0,-2.305285 0.738834,-4.52485 1.77157,-6.496436 -1.053145,0.0745 -2.127721,0.09695 -3.234952,0.09695 -7.261803,0 -13.4286215,-2.351208 -17.99020957,-6.236212 l 0,6.56685 0.00102048,19.70055 C 5.2128523,76.28781 11.409265,74.798916 18.147557,74.798916 z M 44.474145,93.051391 C 43.002599,87.308076 37.788918,84.46091 30.518951,79.420713 c -2.644088,-0.85313 -5.556565,-1.35521 -8.681304,-1.387866 -8.752739,-0.09389 -17.2452523,3.525266 -21.84561906,8.743028 l 0,13.224125 44.64641606,-0.0011 c 0,0 0.263286,-2.21038 0.263286,-3.362512 -10e-4,-1.222547 -0.151032,-2.419581 -0.427585,-3.58498 z"></path></svg></a></li>
</ul>
  
  
  
    
      <form action="http://google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="akhilsbehl.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/categories">Things I've talked about</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title">Why you have, in fact, used a neural network!</h1>
      
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-09-06T17:38:12+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>6</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>5:38 pm</span></time>
        
        
      </p>
    
  </header>


<div class="entry-content"><p>Have you ever used a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>? Would you like to be able to say you have used a <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network</a> – perhaps in your next interview? Here’s your cheat code.</p>

<p><em>NB: <a href="https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch">NFL</a> applies insomuch as you will have to know what we are talking about in this post. This is not a tutorial.</em></p>

<!--more-->

<h2 id="logistic-regression">Logistic regression</h2>

<p>A lot has been said and is known about the logistic regression. Refer <a href="http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf">Chapter 11</a> for a thorough discussion. I’ll attempt a quick recap.</p>

<p>The learning problem is to learn the distribution of a <em>categorical</em> response <script type="math/tex">Y</script> conditional on a set of covariates <script type="math/tex">X</script>, i.e. <script type="math/tex">P[Y = i \mid X = x]</script>. For the <em>binary</em> response case, there are two values that <script type="math/tex">Y</script> can take, e.g. <script type="math/tex">\{\textrm{head}, \textrm{tail}\}</script>, <script type="math/tex">\{\textrm{accept}, \textrm{reject}\}</script>, etc. Traditionally, one of the two responses is encoded as <script type="math/tex">1</script> and the other <script type="math/tex">0</script>. As such, the learning problem is transformed to learning <script type="math/tex">E[Y \mid X = x]</script> which is modeled as a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli outcome</a> with probability of success parametrized as <script type="math/tex">p(X; \theta)</script>. From here, one can write the <em>log <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a></em> under standard assumptions and maximiize it to obtain the parameter estimate <script type="math/tex">\hat{\theta}</script>. In this set up, we have still not chosen the function <script type="math/tex">p(x; \theta)</script>. Different forms of <script type="math/tex">p</script> give rise to different classifiers. One choice – the logistic regression – chooses <script type="math/tex">p</script> to be:</p>

<script type="math/tex; mode=display">p(x; \theta) = \frac{e^{x \bullet \theta}}{1 + e^{x \bullet \theta}}</script>

<h2 id="a-tale-of-two-interpretations">A tale of two interpretations</h2>

<p>Logit regression has been <a href="https://en.wikipedia.org/wiki/Logistic_regression#Formal_mathematical_specification">reinterpreted</a> in innumerable ways. Heck, even this post is a rehashing of one of these interpretations. For our sake, however, we will examine two interpretations and their equivalence.</p>

<h3 id="using-the-log-odds-ratio">Using the log odds-ratio</h3>

<p>The standard intepretation coming from econometrics and the (generalized) linear models camp is the log-odds interpretation. In this interpretation, one linearly models the log of odds-ratio:</p>

<script type="math/tex; mode=display">\ln{\frac{P[Y = 1 \mid X = x]}{P[Y = 0 \mid X = x]}} = \ln\frac{p(x, \theta)}{1 - p(x, \theta)} = x \bullet \theta</script>

<p>Note that rearranging this is equivalent to the defintion of <script type="math/tex">p(x; \theta)</script> given above. However, what is more important for this discussion is to note that <script type="math/tex">p(x; \theta)</script> can be written as a <a href="https://en.wikipedia.org/wiki/Function_composition">function composition</a> – <script type="math/tex">\sigma \bullet</script> – applied on the input vector of covariates (<script type="math/tex">x</script>) and the parameter vector to be learned (<script type="math/tex">\theta</script>). Here <script type="math/tex">\sigma</script> is the <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a> and <script type="math/tex">\bullet</script> is the dot product of two vectors.</p>

<h3 id="using-a-log-linear-model">Using a log-linear model</h3>

<p>Another interpretation of a logistic regression is as a <a href="https://en.wikipedia.org/wiki/Logistic_regression#As_a_.22log-linear.22_model">log-linear</a> model. The difficulty is that the log-linear method is used to estimate outcomes which have support in <script type="math/tex">[0, \infty)</script> whereas we need to model <script type="math/tex">p</script> which is a probability distribution with support in [0, 1]. The trick to overcome this is to model <script type="math/tex">\tilde{p}</script> – an <em>un-normalized</em> probability for each of the binary outcomes as a log-linear model. The true probability <script type="math/tex">p</script> is then realized from <script type="math/tex">\tilde{p}</script> by normalizing it over all possible outcomes.</p>

<script type="math/tex; mode=display">\begin{align}
\ln{\tilde{p}[Y = 0 \mid X = x]} = x \bullet \beta_0 \\
\ln{\tilde{p}[Y = 1 \mid X = x]} = x \bullet \beta_1 \\
\end{align}</script>

<p>Expnonentiating and summing the two <em>un-normalized</em> probabilities, we get (say) <script type="math/tex">Z = e^{x \bullet \beta_0} + e^{x \bullet \beta_1}</script> which we use to find a <em>normalized</em> probability distribution:</p>

<script type="math/tex; mode=display">\begin{align}
P[Y = 0 \mid X = x] = \frac{1}{Z}e^{x \bullet \beta_0} = \frac{e^{x \bullet \beta_0}}{e^{x \bullet \beta_0} + e^{x \bullet \beta_1}} \\
P[Y = 1 \mid X = x] = \frac{1}{Z}e^{x \bullet \beta_1} = \frac{e^{x \bullet \beta_1}}{e^{x \bullet \beta_0} + e^{x \bullet \beta_1}}
\end{align}</script>

<p>Note that in this formulation, I use <script type="math/tex">\beta</script> to denote model parameters and not <script type="math/tex">\theta</script>. This is because the parameter estimates are in fact different. Contrast the probability of <em>success</em> across the two interpretations:</p>

<script type="math/tex; mode=display">P[Y = 1 \mid X = x] = \frac{e^{x \bullet \beta_1}}{e^{x \bullet \beta_0} + e^{x \bullet \beta_1}} = \frac{e^{x \bullet \theta}}{1 + e^{x \bullet \theta}} \\</script>

<p>This arises out of the fact that the log-linear formulation is <a href="https://en.wikipedia.org/wiki/Parameter_identification_problem"><em>over-identified</em></a> because if we know <script type="math/tex">P[Y = 1 \mid X = x]</script>, we automatically know <script type="math/tex">P[Y = 0 \mid X = x]</script>. The over-identification is resolved by setting <script type="math/tex">\beta_0 = 0</script> which reconciles the two expressions above. See <a href="https://en.wikipedia.org/wiki/Logistic_regression#As_a_.22log-linear.22_model">the wiki</a> for an explanation.</p>

<p>Once again, we notice that even this formulation of the logistic regression can be factorized into a function composition where <script type="math/tex">P[Y = i \mid X = x] \equiv p_i(x; \theta) \equiv (s \bullet)_i</script> where <script type="math/tex">s</script> is the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function and <script type="math/tex">\bullet</script> is the dot product.</p>

<h5 id="generalization-to-the-multinomial-case-with-the-softmax-function">Generalization to the multinomial case with the softmax function</h5>

<p>The other advantage to choosing the log-linear formulation is that it is easily generalized to the case of the <em>n-ary</em> categorical variable – equivalent to the <em>multinomial</em> logit case. One can easily replicate the discussion above to derive:</p>

<script type="math/tex; mode=display">P[Y = i \mid X = x] \equiv p_i(x; \theta) = \frac{1}{Z}e^{x \bullet \beta_i} = \frac{e^{x \bullet \beta_i}}{\sum_{k} e^{x \bullet \beta_k}}</script>

<h2 id="nns-as-function-composition-machinery">NNs as function composition machinery</h2>

<p>The class of neural networks that we are going to use here is the simple feed-forward multi-layer perceptron (<a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a>). For a quick recap, a MLP is a directed acyclical graph which has each layer fully-connected to the next layer. The simplest MLP will have at least three layers: the <em>input</em> layer (to which the data is fed), an <em>output</em> layer (which outputs the results) and one (or more for deeper architectures) <em>hidden</em> layer. <a href="http://www.deeplearningbook.org/">Goodfellow et al.</a> (Ch. 1, p. 5) describe:</p>

<blockquote><p>A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by <strong>composing many simpler functions</strong>. We can think of each application of a diﬀerent mathematical function as providing a new representation of the input.</p></blockquote>

<p>Emphasis added.</p>

<p>In fact, in Figure 1.3, the book provides the MLP graph that represents the log-odds formulation of a logistic regression (as the composition <script type="math/tex">\sigma \bullet</script>, further decomposing <script type="math/tex">\bullet</script> into elementary operations <script type="math/tex">\times</script> and <script type="math/tex">+</script>).</p>

<h3 id="mlp-design-of-a-multinomial-logit">MLP design of a multinomial logit</h3>

<p>Similarly, the log-linear interpretation of the logistic regression model can be used to design an n-ary MLP classifier with the following layers:</p>

<ol>
  <li>Input layer of covariates.</li>
  <li>Fully connected hidden layer with n <em>linear units</em> (the <script type="math/tex">\bullet</script> function).</li>
  <li>Fully connected output layer with a <em>softmax unit</em> (the <script type="math/tex">s</script> function).</li>
</ol>

<p>This here is the blueprint of your cheat code promised in the beginning of the post! ;) See this <a href="https://www.youtube.com/watch?v=FYgsztDxSvE">lecture</a> for a full exposition on this design.</p>

<h2 id="tldr">tl;dr</h2>

<p>This is the key insight that motivated this article: NNs are arbitrary function composition machinery that can be tuned to learn model parameters. This was illustrated by decomposing the multinomial logistic regression as a (quite <em>shallow</em>, in fact) composition of functions and then re-assembling it as a MLP. Indeed, <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Bengio</a> (Ch. 2, pp. 15 - 16) summarizes that a diverse set of (machine?) learning techniques such as GLM, kernel machines, decision trees, boosting machines, stacking, and even the human cortex etc. are networks with increasingly complex or <em>deep</em> (à la <em>deep learning</em>) compositions of simple functions.</p>

<!--links-->
</div>


  <footer>
    <p class="meta">
      
  


  


  <span class="byline author vcard">Authored by <span class="fn">
  
    <a href="https://plus.google.com/u/0/101609335408319028285" rel="author">Akhil S. Behl</a>
  
  </span></span>


      




<time class='entry-date' datetime='2016-09-06T17:38:12+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>6</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>5:38 pm</span></time>
      
      

<span class="categories">
  
    <a class='category' href='/blog/categories/classification/'>classification</a>, <a class='category' href='/blog/categories/logistic-regression/'>logistic-regression</a>, <a class='category' href='/blog/categories/machine-learning/'>machine-learning</a>, <a class='category' href='/blog/categories/neural-networks/'>neural-networks</a>, <a class='category' href='/blog/categories/statistics/'>statistics</a>, <a class='category' href='/blog/categories/supervised-learning/'>supervised-learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  <div class="g-plusone" data-size="medium"></div>
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/05/30/inspecting-objects-at-point-with-ess/" title="Previous Post: Inspecting objects at point with ESS">&laquo; Inspecting objects at point with ESS</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/09/07/toggling-buffer-level-hooks-in-emacs/" title="Next Post: Toggling buffer-level hooks in Emacs">Toggling buffer-level hooks in Emacs &raquo;</a>
      
    </p>
  </footer>
</article>


</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Akhil S. Behl -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  








  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>







</body>
</html>
