
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Training with large files in Torch - asb: head /dev/brain > /dev/www</title>
  <meta name="author" content="Akhil S. Behl">

  
  <meta name="description" content="Datasets these days are getting larger and and so is hardware in keeping with Moore’s law. However, Moore’s law applies to population trends. For a &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://akhilsbehl.github.io/blog/2016/09/19/training-with-large-files-in-torch7">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://feeds.feedburner.com/github/Xmaq" rel="alternate" title="asb: head /dev/brain > /dev/www" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-42442765-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">asb: head /dev/brain > /dev/www</a></h1>
  
    <h2>My home, musings, and wanderings on the world wide web.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss">
  <li><a href="http://feeds.feedburner.com/github/Xmaq" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
</ul>
  
  
  
  
  
<ul class="subscribe">
  <li><a href="https://github.com/akhilsbehl" rel="subscribe-github" title="@akhilsbehl on GitHub" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 50,0 C 22.385714,0 0,22.385714 0,50 0,77.614286 22.385714,100 50,100 77.614286,100 100,77.614286 100,50 100,22.385714 77.614286,0 50,0 z m 29.692858,79.692858 c -3.859184,3.859182 -8.351022,6.887754 -13.35,9.00306 -1.27041,0.536736 -2.560204,1.009184 -3.867348,1.415306 v -7.493878 c 0,-3.938774 -1.35102,-6.835714 -4.053062,-8.690816 1.692858,-0.163264 3.24694,-0.390816 4.663266,-0.683672 1.416326,-0.292858 2.913266,-0.716328 4.491838,-1.27041 1.57857,-0.55408 2.994896,-1.213264 4.247958,-1.97755 1.253062,-0.765306 2.458164,-1.758164 3.613266,-2.978572 1.155102,-1.220408 2.12449,-2.604082 2.905102,-4.15 0.780612,-1.545918 1.4,-3.40204 1.855102,-5.566326 0.455102,-2.164286 0.683674,-4.54898 0.683674,-7.153062 0,-5.045918 -1.643878,-9.341836 -4.931634,-12.890816 C 77.44796,33.35 77.285714,29.10204 75.463266,24.512244 l -1.22143,-0.145918 c -0.845918,-0.09796 -2.368366,0.260204 -4.565306,1.07449 -2.196938,0.814286 -4.663264,2.14796 -7.396938,4.004082 -3.87449,-1.07449 -7.893878,-1.611224 -12.061224,-1.611224 -4.19898,0 -8.203062,0.536734 -12.012246,1.611224 -1.72449,-1.17245 -3.361224,-2.139796 -4.907142,-2.905102 C 31.753062,25.77449 30.516326,25.254082 29.587756,24.97653 28.660204,24.7 27.79796,24.528572 27,24.463266 c -0.79796,-0.0653 -1.310204,-0.08062 -1.537756,-0.04898 -0.22755,0.03164 -0.390816,0.0653 -0.487754,0.09796 -1.82347,4.62245 -1.985714,8.87143 -0.487756,12.743878 -3.287754,3.54796 -4.931632,7.844898 -4.931632,12.890816 0,2.604082 0.227552,4.988776 0.683674,7.153062 0.456122,2.164286 1.07449,4.020408 1.855102,5.566326 0.780612,1.545918 1.75,2.929592 2.905102,4.15 1.155102,1.220408 2.360204,2.213266 3.613264,2.978572 1.253062,0.766326 2.669388,1.42449 4.24796,1.97755 1.578572,0.554082 3.07551,0.976532 4.491836,1.27041 1.416328,0.292856 2.970408,0.521428 4.663266,0.683672 -2.669388,1.82347 -4.004082,4.720408 -4.004082,8.690816 v 7.639796 C 36.536734,89.818368 35.083674,89.3 33.656122,88.695918 c -4.99898,-2.115306 -9.490816,-5.143878 -13.35,-9.00306 -3.859184,-3.859184 -6.887754,-8.351022 -9.00306,-13.35 C 9.1163263,61.171428 8.0071428,55.67347 8.0071428,50 c 0,-5.67347 1.1091835,-11.171428 3.2969392,-16.342858 2.115306,-4.998978 5.143878,-9.490816 9.00306,-13.35 3.859184,-3.859182 8.351022,-6.887754 13.35,-9.00306 C 38.828572,9.1163266 44.32653,8.0071428 50,8.0071428 c 5.67347,0 11.171428,1.1091838 16.342858,3.2969392 5,2.115306 9.490816,5.143878 13.35,9.00306 3.859182,3.859184 6.887754,8.351022 9.00306,13.35 2.186736,5.17245 3.295918,10.67041 3.295918,16.342858 0,5.672448 -1.109182,11.171428 -3.296938,16.342858 -2.115306,4.998978 -5.143878,9.490816 -9.00204,13.35 l 0,0 z"></path></svg></a></li>
</ul>
  
  
  
  
<ul class="subscribe">
  <li><a href="https://linkedin.com/in/akhilsbehl" rel="subscribe-linkedin" title="Akhil S. Behl on LinkedIn" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewBox="0 0 731 1000"><path class="social" d="M158.75 865l-156.25 0l0 -521.25l156.25 0l0 521.25zm393.75 -505q87.5 0 133.125 56.25t45.625 153.75l0 295l-156.25 0l0 -278.75q0 -108.75 -76.25 -108.75 -61.25 0 -80 61.25l0 326.25l-156.25 0q2.5 -468.75 0 -521.25l123.75 0l10 103.75l2.5 0q53.75 -87.5 153.75 -87.5zm-552.5 -146.25q0 -78.75 81.25 -78.75 80 0 80 78.75 0 77.5 -80 77.5 -81.25 0 -81.25 -77.5z"/></svg></a></li>
</ul>
  
  
<ul class="subscribe">
  <li><a href="http://stackoverflow.com/users/1533691" rel="subscribe-stackoverflow" title="Akhil S. Behl on StackOverflow" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewBox="0 0 64 64"><g
     id="layer1" class="social">
    <path
       d="m 9.3049611,36.847632 4.4013079,0.04316 -0.153442,19.598393 29.291259,0 0,-19.527506 4.637782,0 0,24.287331 -38.2006795,0 0.023777,-24.401371 z"
       id="path2830"
       style="fill-opacity:1;stroke:none"/>
    <rect
       width="22.944817"
       height="4.881876"
       x="16.481951"
       y="48.434078"
       id="rect3604"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="20.749949"
       y="37.830307"
       transform="matrix(0.9953749,0.09606666,-0.09606666,0.9953749,0,0)"
       id="rect3606"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="26.472515"
       y="23.401554"
       transform="matrix(0.96240291,0.27162592,-0.27162592,0.96240291,0,0)"
       id="rect3606-1"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="30.528769"
       y="3.1535864"
       transform="matrix(0.85597805,0.51701216,-0.51701216,0.85597805,0,0)"
       id="rect3606-1-3"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="27.191883"
       y="-24.475019"
       transform="matrix(0.58242689,0.81288309,-0.81288309,0.58242689,0,0)"
       id="rect3606-1-3-7"
       style="fill-opacity:1;stroke:none" />
    <rect
       width="23.066864"
       height="5.0039229"
       x="11.174569"
       y="-50.183693"
       transform="matrix(0.16480989,0.98632535,-0.98632535,0.16480989,0,0)"
       id="rect3606-1-3-7-6"
       style="fill-opacity:1;stroke:none" />
  </g></svg></a></li>
</ul>
  
  
  
  
<ul class="subscribe">
  <li><a href="https://plus.google.com/u/0/101609335408319028285" rel="publisher" title="Google+ Profile" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 23.03264,55.19021 C 32.01805,55.457578 38.046082,46.137447 36.495958,34.370183 34.943794,22.603939 26.398215,13.161349 17.411784,12.89296 8.4253533,12.625592 2.3973217,21.630392 3.9494863,33.400718 5.5006303,45.164921 14.043148,54.921821 23.03264,55.19021 z M 99.99898,24.999027 100,0 -0.00797206,0 0.0052943,16.202408 C 5.7047282,11.184661 13.611481,6.9914696 21.771315,6.9914696 c 8.721103,0 34.888495,0 34.888495,0 l -7.807765,6.6035874 -11.062106,0 c 7.33732,2.81349 11.246815,11.3407 11.246815,20.090377 0,7.348545 -4.082979,13.667416 -9.852826,18.161652 -5.629021,4.385043 -6.696453,6.220904 -6.696453,9.948752 0,3.180866 6.030073,8.594563 9.183386,10.81923 9.217061,6.498477 12.199952,12.530591 12.199952,22.603843 0,1.604209 -0.198996,3.206378 -0.591884,4.780993 L 100,100 c 0,0 0,-46.594917 0,-68.751495 l -18.750474,0 0,18.750475 -6.250499,0 0,-18.750475 -18.750474,0 0,-6.250498 18.750474,0 0,-18.7494538 6.250499,0 0,18.7494538 18.750474,0 z M 18.147557,74.798916 c 2.111393,0 4.04522,-0.05817 6.048441,-0.05817 -2.651232,-2.571634 -4.749358,-5.722905 -4.749358,-9.606889 0,-2.305285 0.738834,-4.52485 1.77157,-6.496436 -1.053145,0.0745 -2.127721,0.09695 -3.234952,0.09695 -7.261803,0 -13.4286215,-2.351208 -17.99020957,-6.236212 l 0,6.56685 0.00102048,19.70055 C 5.2128523,76.28781 11.409265,74.798916 18.147557,74.798916 z M 44.474145,93.051391 C 43.002599,87.308076 37.788918,84.46091 30.518951,79.420713 c -2.644088,-0.85313 -5.556565,-1.35521 -8.681304,-1.387866 -8.752739,-0.09389 -17.2452523,3.525266 -21.84561906,8.743028 l 0,13.224125 44.64641606,-0.0011 c 0,0 0.263286,-2.21038 0.263286,-3.362512 -10e-4,-1.222547 -0.151032,-2.419581 -0.427585,-3.58498 z"></path></svg></a></li>
</ul>
  
  
  
    
      <form action="http://google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="akhilsbehl.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/categories">Things I've talked about</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title">Training with large files in Torch</h1>
      
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-09-19T17:50:12+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>5:50 pm</span></time>
        
        
      </p>
    
  </header>


<div class="entry-content"><p>Datasets these days are getting larger and and so is hardware in keeping with <a href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore’s law</a>. However, Moore’s law applies to population trends. For a given user, budget constraints create a situation where hardware resources increase as a step function. On the other hand, the growth function for datasizes can be approximated with a much smoother exponential. This keeps users typically chasing their own hyper-personal version of <a href="https://en.wikipedia.org/wiki/Big_data">big data</a>.</p>

<p>I recently started using <a href="https://en.wikipedia.org/wiki/Lua_(programming_language)">lua</a> and <a href="http://torch.ch/">torch</a> to learn about neural networks. I wanted to start simple and build a classifier for a kaggle competition. However, the data for this competition is too <em>big</em> for my machine with a measly 16 gigs of RAM. [This used to be a luxury only half a decade ago.]</p>

<p>So, after some digging around on github, I figured how to train with fairly large datasets with stock torch infrastructure and I’m gonna show you how and why it works.</p>

<!--more-->

<h1 id="how-did-you-do-that">How did you do that?</h1>

<h2 id="hold-up-what-all-did-you-try">Hold up, what all did you try?</h2>

<h3 id="csv2tensor">csv2tensor</h3>

<p>The file that I have is 2 gigs on disk. So, my first attempt, obviously, was to throw caution to the wind and attempt to load the dataset anyway. To this end, I tried <a href="https://github.com/willkurt/csv2tensor">csv2tensor</a>. This was a no-go right from the get-go (you see what I did there?).</p>

<p>This library converts the columns in the data into lua <code>tables</code> and then converts those <code>tables</code> to torch <code>Tensors</code>. The rub is that tables in lua are not allocated in user memory but in the compiler’s memory which has a small <a href="http://kvitajakub.github.io/2016/03/08/luajit-memory-limitations/">upper-bound</a> because in <a href="http://luajit.org/">luajit</a> that torch is typically compiled against. This means that using this library will blow up for any dataset which expands to a more than a gig or so.</p>

<h3 id="csvigo">csvigo</h3>

<p><a href="https://github.com/clementfarabet/lua---csv/"><code>csvigo</code></a> is the standard library for working with csv files in lua / torch7. Again, my first attempt was to read the whole data into memory with this package. I opened <code>top</code> in a split and proceeded to load the dataset. While this library did not run into the issue above (I think because it uses <code>Tensors</code> directly which are allocated in user-space), it quickly ate up all the available memory and a few gigs of swap before I killed the process. I tried to <code>setdefaulttensortype</code> to <code>torch.FloatTensor</code> which I killed after waiting for under a minute by when it had clocked 7 gigs.</p>

<h2 id="the-solution-csvigo-with-emmodelargeem">The solution: csvigo with <em>mode=large</em></h2>

<p>At this point, I looked at the csvigo documentation again and found the <a href="https://github.com/clementfarabet/lua---csv/#large-csv-mode">large-mode</a> option. I decided to try it and was able to successfully put together this solution:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="kd">local</span> <span class="n">numericData</span> <span class="o">=</span> <span class="n">csvigo</span><span class="p">.</span><span class="n">load</span><span class="p">({</span>
</span><span class="line">      <span class="n">path</span> <span class="o">=</span> <span class="n">dataDir</span> <span class="o">..</span> <span class="s2">&quot;</span><span class="s">numeric.csv&quot;</span><span class="p">,</span>
</span><span class="line">      <span class="n">mode</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s">large&#39;</span>
</span><span class="line"><span class="p">})</span>
</span><span class="line">
</span><span class="line"><span class="c1">-- Use a level of indirection to get at the records</span>
</span><span class="line"><span class="kd">local</span> <span class="n">trainingData</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">
</span><span class="line"><span class="c1">-- Discount 1 for the header row</span>
</span><span class="line"><span class="k">function</span> <span class="nf">trainingData</span><span class="p">:</span><span class="n">size</span><span class="p">()</span> <span class="k">return</span> <span class="o">#</span><span class="n">numericData</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">end</span>
</span><span class="line">
</span><span class="line"><span class="cm">--[[</span>
</span><span class="line"><span class="cm">`preprocess` is an arbitrary function that operates on a row of data.</span>
</span><span class="line"><span class="cm">Implement it (or remove it) as per your requirements.</span>
</span><span class="line"><span class="cm">Assumption: The response is at the last index in the record</span>
</span><span class="line"><span class="cm">--]]</span>
</span><span class="line"><span class="nb">setmetatable</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span>
</span><span class="line">            <span class="p">{</span>
</span><span class="line">                <span class="n">__index</span> <span class="o">=</span> <span class="k">function</span> <span class="p">(</span><span class="n">tbl</span><span class="p">,</span> <span class="n">ind</span><span class="p">)</span>
</span><span class="line">                   <span class="kd">local</span> <span class="n">row</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">numericData</span><span class="p">[</span><span class="n">ind</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span><span class="line">                   <span class="kd">local</span> <span class="n">record</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
</span><span class="line">                   <span class="kd">local</span> <span class="n">response</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line">                   <span class="kd">local</span> <span class="n">penultimate</span> <span class="o">=</span> <span class="n">record</span><span class="p">:</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class="line">                   <span class="kd">local</span> <span class="n">rest</span> <span class="o">=</span> <span class="n">record</span><span class="p">[{</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="n">penultimate</span><span class="p">}</span> <span class="p">}]</span>
</span><span class="line">                   <span class="k">return</span> <span class="p">{</span><span class="n">rest</span><span class="p">,</span> <span class="n">response</span><span class="p">}</span>
</span><span class="line">                <span class="k">end</span>
</span><span class="line"><span class="p">});</span>
</span><span class="line">
</span><span class="line"><span class="c1">-- Trainer</span>
</span><span class="line"><span class="c1">----------</span>
</span><span class="line">
</span><span class="line"><span class="c1">-- Let `mlp` be our nn model</span>
</span><span class="line">
</span><span class="line"><span class="kd">local</span> <span class="n">params</span><span class="p">,</span> <span class="n">gradParams</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">:</span><span class="n">getParameters</span><span class="p">()</span>
</span><span class="line"><span class="kd">local</span> <span class="n">optimState</span> <span class="o">=</span> <span class="p">{</span><span class="n">learningRate</span> <span class="o">=</span> <span class="mi">1</span><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="c1">-- Training</span>
</span><span class="line"><span class="c1">-----------</span>
</span><span class="line">
</span><span class="line"><span class="kd">local</span> <span class="n">nCols</span> <span class="o">=</span> <span class="o">#</span><span class="p">(</span><span class="n">numericData</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class="line"><span class="kd">local</span> <span class="n">nEpochs</span> <span class="o">=</span> <span class="mi">32</span>
</span><span class="line"><span class="kd">local</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">2048</span>
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nEpochs</span> <span class="k">do</span>
</span><span class="line">
</span><span class="line">   <span class="kd">local</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">trainingData</span><span class="p">:</span><span class="n">size</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">   <span class="kd">local</span> <span class="n">batch</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">   <span class="k">for</span> <span class="n">batchOffset</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trainingData</span><span class="p">:</span><span class="n">size</span><span class="p">(),</span> <span class="n">batchSize</span> <span class="k">do</span>
</span><span class="line">
</span><span class="line">      <span class="kd">local</span> <span class="n">batchInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">nCols</span><span class="p">)</span>
</span><span class="line">      <span class="kd">local</span> <span class="n">batchResponse</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">batchSize</span><span class="p">)</span>
</span><span class="line">      <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batchSize</span> <span class="k">do</span>
</span><span class="line">         <span class="kd">local</span> <span class="n">thisRecord</span> <span class="o">=</span> <span class="n">trainingData</span><span class="p">[</span><span class="n">shuffle</span><span class="p">[</span><span class="n">batchOffset</span> <span class="o">+</span> <span class="n">i</span><span class="p">]]</span>
</span><span class="line">         <span class="n">batchInputs</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">copy</span><span class="p">(</span><span class="n">thisRecord</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">         <span class="n">batchResponse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">thisRecord</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">
</span><span class="line">      <span class="kd">local</span> <span class="k">function</span> <span class="nf">evaluateBatch</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</span><span class="line">         <span class="n">gradParams</span><span class="p">:</span><span class="n">zero</span><span class="p">()</span>
</span><span class="line">         <span class="kd">local</span> <span class="n">batchEstimate</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">batchInputs</span><span class="p">)</span>
</span><span class="line">         <span class="kd">local</span> <span class="n">batchLoss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">batchEstimate</span><span class="p">,</span> <span class="n">batchResponse</span><span class="p">)</span>
</span><span class="line">         <span class="kd">local</span> <span class="n">nablaLossOutput</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span>
</span><span class="line">            <span class="n">batchEstimate</span><span class="p">,</span> <span class="n">batchResponse</span><span class="p">)</span>
</span><span class="line">         <span class="n">mlp</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">batchInputs</span><span class="p">,</span> <span class="n">nablaLossOutput</span><span class="p">)</span>
</span><span class="line">         <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">Finished epoch: &#39;</span> <span class="o">..</span> <span class="n">epoch</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">, batch: &#39;</span> <span class="o">..</span>
</span><span class="line">                  <span class="n">batch</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">, with loss: &#39;</span> <span class="o">..</span> <span class="n">batchLoss</span><span class="p">)</span>
</span><span class="line">         <span class="k">return</span> <span class="n">batchLoss</span><span class="p">,</span> <span class="n">gradParams</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">
</span><span class="line">      <span class="n">optim</span><span class="p">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">evaluateBatch</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">optimState</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">      <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span> <span class="o">+</span> <span class="mi">1</span>
</span><span class="line">      <span class="n">batchOffset</span> <span class="o">=</span> <span class="n">batchOffset</span> <span class="o">+</span> <span class="n">batchSize</span>
</span><span class="line">
</span><span class="line">   <span class="k">end</span>
</span><span class="line">
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>With this method I was able to use the dataset with only two gigs of memory allocated to the process. Perhaps, in another post I will benchmark the performance penalty for doing this when you do have sufficient RAM. I have also not figured out how this will need to be adapted to GPUs. But this is definitely better compared to the option of not using a moderately large dataset at all.</p>

<h2 id="whats-the-magic-sauce-oo">What’s the magic sauce? o.O</h2>

<p>Note how the <a href="https://github.com/clementfarabet/lua---csv/#large-csv-mode">documentation</a> modestly states that the efficiency is under the hood? Which is why we need to <a href="http://www.catb.org/jargon/html/U/UTSL.html"><em>use the source, Luke</em></a>. So, let’s look at this (incomplete) <a href="https://github.com/clementfarabet/lua---csv/blob/master/File.lua">code</a> snippet and comprehend:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Csv</span><span class="p">:</span><span class="n">largereadall</span><span class="p">()</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">ok</span> <span class="o">=</span> <span class="nb">pcall</span><span class="p">(</span><span class="nb">require</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">torch&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="ow">not</span> <span class="n">ok</span> <span class="k">then</span>
</span><span class="line">        <span class="nb">error</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">large mode needs the torch package&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">end</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">libcsvigo</span> <span class="o">=</span> <span class="nb">require</span> <span class="s1">&#39;</span><span class="s">libcsvigo&#39;</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">ffi</span> <span class="o">=</span> <span class="nb">require</span> <span class="s1">&#39;</span><span class="s">ffi&#39;</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">path</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">filepath</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">DiskFile</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">r&#39;</span><span class="p">):</span><span class="n">binary</span><span class="p">()</span>
</span><span class="line">    <span class="n">f</span><span class="p">:</span><span class="n">seekEnd</span><span class="p">()</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">length</span> <span class="o">=</span> <span class="n">f</span><span class="p">:</span><span class="n">position</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class="line">    <span class="n">f</span><span class="p">:</span><span class="n">seek</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="p">:</span><span class="n">readChar</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
</span><span class="line">    <span class="n">f</span><span class="p">:</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="c1">-- now that the ByteStorage is constructed,</span>
</span><span class="line">    <span class="c1">-- one has to make a dictionary of [offset, length] pairs of the row.</span>
</span><span class="line">    <span class="c1">-- for efficiency, do one pass to count number of rows,</span>
</span><span class="line">    <span class="c1">-- and another pass to create a LongTensor and fill it</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">lookup</span> <span class="o">=</span> <span class="n">libcsvigo</span><span class="p">.</span><span class="n">create_lookup</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="kd">local</span> <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">    <span class="kd">local</span> <span class="n">separator</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">separator</span>
</span><span class="line">
</span><span class="line">    <span class="kd">local</span> <span class="k">function</span> <span class="nf">index</span> <span class="p">(</span><span class="n">tbl</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span><span class="line">        <span class="nb">assert</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">index has to be given&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="nb">assert</span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">lookup</span><span class="p">:</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;</span><span class="s">index out of bounds: &quot;</span> <span class="o">..</span>  <span class="n">i</span><span class="p">)</span>
</span><span class="line">        <span class="kd">local</span> <span class="n">line</span> <span class="o">=</span> <span class="n">ffi</span><span class="p">.</span><span class="n">string</span><span class="p">(</span><span class="n">data</span><span class="p">:</span><span class="n">data</span><span class="p">()</span> <span class="o">+</span> <span class="n">lookup</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">lookup</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
</span><span class="line">        <span class="kd">local</span> <span class="n">entry</span> <span class="o">=</span> <span class="n">fromcsv</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">separator</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">entry</span>
</span><span class="line">    <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<ol>
  <li>
    <p><strong>Loading to ByteStorage</strong>: The first interesting thing that the code does is to load the file as raw bytes. This is the part of code spanning <code>torch.DiskFile(path, 'r'):binary()</code> to <code>f:close()</code>. The intermediate steps are only to calculate the length of the content in bytes. This makes the data take only as much space in memory as it does on the disk (remember how my 2GB file took only 2 gigs in RAM?).</p>
  </li>
  <li>
    <p><strong>Calculating the byte offset and the byte length of each new record in the file</strong>: The <code>libcsvigo.create_lookup(data)</code> essentially scans the byte representation of the file and records at what offset from the beginning is the first byte of each record. Additionally, it also stores how many bytes are in each record – the byte length. These are stored as pairs in the <code>lookup</code> table.</p>
  </li>
  <li>
    <p><strong>Accessing the <code>i</code>th record in the file</strong>: When it’s time to access the <code>i</code>th row in the file, the function <code>index</code> creates a string by reading in <code>lookup[i][2]</code> number of bytes starting from <code>lookup[i][1]</code>. The <code>fromcsv</code> function then splits this record string into a <code>table</code> by splitting it on the separator and returns it us.</p>
  </li>
</ol>

<p>This allows us to load large datasets in memory employing only as much memory as is the size of the file on disk (in bytes) with the additional memory cost of creating the <code>LongTensor</code> lookup table. There is, obviously, the additional CPU cost of processing each record as many times as it is read.</p>

<!--links-->
</div>


  <footer>
    <p class="meta">
      
  


  


  <span class="byline author vcard">Authored by <span class="fn">
  
    <a href="https://plus.google.com/u/0/101609335408319028285" rel="author">Akhil S. Behl</a>
  
  </span></span>


      




<time class='entry-date' datetime='2016-09-19T17:50:12+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>5:50 pm</span></time>
      
      

<span class="categories">
  
    <a class='category' href='/blog/categories/big-data/'>big-data</a>, <a class='category' href='/blog/categories/io/'>io</a>, <a class='category' href='/blog/categories/lua/'>lua</a>, <a class='category' href='/blog/categories/torch/'>torch</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  <div class="g-plusone" data-size="medium"></div>
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/09/07/toggling-buffer-level-hooks-in-emacs/" title="Previous Post: Toggling buffer-level hooks in Emacs">&laquo; Toggling buffer-level hooks in Emacs</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/10/12/hackerrank-implementing-a-queue-using-two-stacks/" title="Next Post: HackerRank: Implementing a queue using two stacks.">HackerRank: Implementing a queue using two stacks. &raquo;</a>
      
    </p>
  </footer>
</article>


</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Akhil S. Behl -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  








  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>







</body>
</html>
